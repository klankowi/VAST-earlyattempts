---
title: "**Groundfish Survey Time Series Correlations**"
geometry: "left=0.75in,right=0.75in,top=1in,bottom=1in"
output:
  bookdown::pdf_document2:
    toc: no
    number_sections: false
urlcolor: blue
documentclass: article
header-includes:
    - \usepackage{setspace}\onehalfspacing
    - \usepackage{float}
    - \usepackage{fancyhdr}
    - \pagestyle{fancy}
fontsize: 12pt
sansfont: Calibri
---

```{r setup, include=FALSE, warning=F, message=F}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
# Clear workspace
rm(list=ls())

#### Load libraries ####
library(tidyverse, quietly=T,verbose=F)
library(ggh4x, quietly=T, verbose=F)
library(tseries, quietly=T, verbose=F)
library(forecast, quietly=T, verbose=F)
library(feasts, quietly = T, verbose=F)
library(lmtest, quietly = T, verbose=F)
library(padr, quietly=T, verbose=F)
library(astsa, quietly=T, verbose=F)
library(kableExtra, quietly=T, verbose=F)
library(grid, quietly=T, verbose=F)
library(gridExtra, quietly=T, verbose=F)
library(ggpubr, quietly=T, verbose=F)
library(scales, quietly=T, verbose=F)

library(sf, quietly=T, verbose=F)
library(BAMMtools, quietly=T, verbose=F)
library(viridis, quietly=T, verbose=F)
library(spData, quietly=T, verbose=F)

# Set seed for reproducibility
set.seed(123)

# Load functions
source(paste0("C:/Users/klankowicz/Box/Katie Lankowicz/",
                        "Groundfish_Survey_Data_Mods/R_Code/",
                        "seasonal_cor_function.R"))
source(paste0("C:/Users/klankowicz/Box/Katie Lankowicz/",
                        "Groundfish_Survey_Data_Mods/R_Code/",
                        "stockseason_cor_function.R"))
source(paste0("C:/Users/klankowicz/Box/Katie Lankowicz/",
                        "Groundfish_Survey_Data_Mods/R_Code/",
                        "stockseason_tsplot_function.R"))
```

```{r, warning=F, message=F, echo=F}
# Load data
indd <- read.csv(paste0("C:/Users/klankowicz/Box/Katie Lankowicz/",
                        "Groundfish_Survey_Data_Mods/Collated_Data/",
                        "Index_Data2.csv"))

# Remove data before 1982 (not used in assessment)
indd <- subset(indd, YEAR > 1981)

# Remove recruitment indices
indd <- subset(indd, AGES!='A0')

# Remove extra DFO series
indd <- subset(indd, INDEX_NAME != "5Z1-2")
indd <- subset(indd, INDEX_NAME != "5Z1-7")
indd$INDEX_NAME[indd$INDEX_NAME =='5ZJ-M'] <- 'GBK'

# Fix SMAST series names
indd$SURVEY[indd$SURVEY == "SMAST Video Trawl" &
            indd$INDEX_NAME == "JEFFRIES"] <- "SMAST Video Trawl Jeffries"
indd$SURVEY[indd$SURVEY == "SMAST Video Trawl" &
            indd$INDEX_NAME == "STELLWAGEN"] <- "SMAST Video Trawl Stellwagen"
indd$SURVEY[indd$SURVEY == "SMAST Video Trawl" &
            indd$INDEX_NAME == "WGOM"] <- "SMAST Video Trawl WGOM"

# Create naming conventionS
indd$NAME <- paste0(indd$SURVEY, " ", indd$SEASON, " ", indd$INDEX_NAME)
indd$STOCKSEASON <- paste(indd$STOCK,indd$SEASON)
indd.half <- split(indd, f=indd$STOCKSEASON)
for(i in 1:length(indd.half)){
  indd.half[[i]]$NAME <- as.factor(indd.half[[i]]$NAME)
  indd.half[[i]]$NAME <- droplevels(indd.half[[i]]$NAME)
  indd.half[[i]]$PSYCH <- as.numeric(as.factor(indd.half[[i]]$NAME))
}
indd2 <- do.call(rbind, indd.half)
row.names(indd2) <- NULL

# Convert index to numeric
indd$INDEX <- as.numeric(indd$INDEX)

# Set colors
survs <- unique(indd$SURVEY)
survs <- c(survs[1:3], "RIDEM Trawl",
           survs[4:12])
hues <- hue_pal()(11)
set.seed(345)
hues <- sample(hues)
hues <- c(hues[1], "black",
          hues[2:9], "brown",
          hues[10:11])
surcols <- setNames(hues, survs)

## Add spatial information
# Load sediment shapefile
seds <- st_read(paste0("C:/Users/klankowicz/Downloads/",
                       "Fishing_Effects_Sediment/",
                       "Fishing_Effects_Sediment/FishingEffectsSediment.shp"),
                quiet = TRUE)

## Load survey data
surveys <- read.csv(paste0("C:/Users/klankowicz/Box/Katie Lankowicz/",
                           "Groundfish_Survey_Data_Mods/Collated_Data/",
                           "Survey_Data.csv"))
surveys <- dplyr::select(surveys, LAT, LON, YEAR, SEASON, STOCK, SURVEY,
                         COD_N, BOTTOM.TYPE, STRATUM)
surveys <- subset(surveys, STRATUM!='5Z7' &
                    STRATUM !="5Z6" &
                    STRATUM !="5Z5")

# Create gear type
surveys$GEAR <- NA
for(i in 1:nrow(surveys)){
  if(surveys$SURVEY[i] == 'NEFSC BLLS'){
    surveys$GEAR[i] <- 'Longline'
  }
  if(surveys$SURVEY[i] == 'Sentinel'){
    surveys$GEAR[i] <- 'Jigging'
  }
  if(surveys$SURVEY[i] !='NEFSC BLLS' &
     surveys$SURVEY[i] !='Sentinel'){
    surveys$GEAR[i] <- 'Trawl'
  }
}

# Remove NA values
surveys <- subset(surveys, is.na(LAT)==FALSE)
surveys <- subset(surveys, is.na(LON)==FALSE)

# Convert to SF object
surveys_sf <- st_as_sf(surveys, coords=c("LON", "LAT"))

# Set CRS
st_crs(surveys_sf) <- "EPSG: 4326"
surveys_sf <- st_transform(surveys_sf,
                        crs=st_crs(seds))

# Set bounding box, transform
oldbbox <- st_bbox(surveys_sf)

# Transform old bounding box
newbbox <-  st_bbox(
  st_transform(
    st_as_sfc(oldbbox), 
    9001
  )
)

# Assign bounding box
attr(st_geometry(surveys_sf), "bbox") <- newbbox
attr(st_geometry(seds), "bbox") <- newbbox

## Load states
us_states <- st_read(paste0("C:/Users/klankowicz/Downloads/",
                       "s_22mr22/",
                       "s_22mr22.shp"),
                quiet = TRUE)

# Transform
us_states_sf <- st_transform(us_states,
                             crs=st_crs(seds))
# Set bounding box
attr(st_geometry(us_states_sf), "bbox") <- newbbox

## Load DFO Strata
dfo_strat <- st_read(paste0(
  "C:/Users/klankowicz/Downloads/",
  "Fishing_Effects_Sediment/",
  "Fishing_Effects_Sediment/DFO_Strata.shp"),
  quiet = TRUE)

# Convert to SF object, transform, set bbox
dfo_sf <- st_as_sf(dfo_strat)
dfo_sf <- st_transform(dfo_sf,
                        crs=st_crs(seds))
attr(st_geometry(dfo_sf), "bbox") <- newbbox

dfo <- subset(surveys_sf, SURVEY=="DFO Trawl")
dfo <- st_transform(dfo,
                        crs=st_crs(seds))

# Remove DFO points not in 5Zejm area
test <- st_intersection(dfo_sf, dfo)
test$Id <- NULL
test <- subset(test, SEASON!="SUMMER")

# Subset out DFO points
surveys_sf <- subset(surveys_sf, SURVEY !="DFO Trawl")

# Subset out NEFSC BTS points
nefsc <- subset(surveys_sf, SURVEY=="NEFSC BTS")
surveys_sf <- subset(surveys_sf, SURVEY!="NEFSC BTS")

# Remove points that are on land
surveys_sf2 <- st_intersection(surveys_sf, seds)
surveys_sf <- dplyr::select(surveys_sf2, YEAR, SEASON, STOCK, SURVEY, COD_N,
                            BOTTOM.TYPE, STRATUM, GEAR, geometry)

# Add back DFO points
surveys_sf <- rbind(surveys_sf, test)
surveys_sf <- rbind(surveys_sf, nefsc)

rm(indd.half, survs, hues, i, dfo, dfo_sf, dfo_strat, nefsc, seds,
   surveys, surveys_sf2, test, us_states, newbbox, oldbbox)

``` 

# Introduction
  
|        Eleven surveys of groundfish abundance are currently available for use. These surveys cover four stock areas (Western Gulf of Maine, Eastern Gulf of Maine, Georges Bank, and Southern New England) and are taken over multiple temporal periods. Most surveys are conducted twice annually, usually in the spring and fall. Uncommonly, surveys sample monthly or weekly and the stratified mean indices of abundance provided are cumulative for the year. It is of interest to compare the indices resulting from these surveys and test for correlations.
\smallskip
|        Background work must first be conducted to make the indices comparable at all, since the methods of calculating a stratified mean index of abundance may not be the same for all surveys. Z-scores are used to normalize the indices.
\smallskip
|        Being time series, correlation among and between indices must be checked across the possible time lags. Commonly, the cross-correlation function (CCF) or dynamic time warping are used to do this. We will use the cross-correlation function for this analysis. CCF is impacted by dependence within a time series, so this dependence must be removed to make meaningful comparisons. For this reason, the z-score normalized indices are modeled using \texttt{auto.arima()} from the \texttt{forecast} package to remove the within-series dependence. The residuals from ARIMA-modeled time series will be tested for correlations.
\smallskip
|        Correlation between and among indices will be tested in two ways. First, we will compare indices taken by the same survey in the same area across different seasons (for example, comparison of the spring and fall NEFSC BTS Western Gulf of Maine indices). Then, we will compare all indices resulting from surveys conducted in the same stock area within the same temporal season (for example, comparison of all Fall Western Gulf of Maine indices). A few surveys produce indices that have no direct comparisons; for example, the ASMFC Northern Shrimp Trawl produces only a Summer Western Gulf of Maine index. No other survey produces indices in this season. Indices with no direct comparison cannot be considered in correlation tests.
  
\newpage

# Data cleaning and coverage
    
|        This analysis will focus on 1982 - 2022. There are limited data before 1982, but the historical period of interest begins at 1982. Many surveys do not yet have data available for 2022, and several surveys were disrupted in 2020 by the pandemic. Note that several indices only have 1-3 years of information (i.e., SMAST Video Trawl Jeffries). These very short time series are difficult to normalize, model, and compare. They also are unlikely to add much information to the overall dataset. Therefore, whey will be removed from analysis.
  
```{r, warning=F, message=F, echo=F, fig.width=7.75, fig.height=7}
# Plot to see coverage
indd2$SURVEY[indd2$SURVEY == 'MADMF Trawl' &
               indd2$AGES == "A0"] <- 'MADMF Trawl Recruitment'


indd2$PSYCH2 <- as.numeric(as.factor(indd2$NAME))
p <- ggplot(indd2, aes(YEAR, PSYCH, col=SURVEY)) + 
  geom_line(lwd=2) +
  scale_color_manual(values=surcols[unique(indd2$SURVEY)]) +
  xlim(1982, 2022) +
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        panel.spacing = unit(1, "lines"),
        axis.line = element_line(colour = "black"),
        legend.position = "bottom",
        legend.title=element_blank(),
        strip.text.x=element_text(size=14),
        axis.text.x=element_text(size=11),
        axis.text.y=element_text(size=11),
        axis.title.x=element_text(size=14)) + 
    guides(color=guide_legend(nrow=4)) +
  xlab('Year') +
  ylab(' ') + 
  facet_wrap2(vars(indd2$STOCKSEASON))
plot(p)

rm(indd2, p)
```
  
# Features of cod indices of abundance
  
|        It is useful to visualize time series prior to analysis to get a rough sense of their features. Here, we will plot z-score normalized indices of abundance to get a look at the trends through time, grouped by stock area and season.
 
```{r, message=F, warning=F, echo=F, fig.show="hold", fig.width=7, fig.height=9}
inddo.list <- split(indd, f=indd$NAME)

# Remove indices with fewer than 4 data points (cannot do z-score, really)
inddo.list <- Filter(function(dt) nrow(dt) > 3, inddo.list)

# Normalize with z-score
for(i in 1:length(inddo.list)){
  inddo.list[[i]]$INDEX.Z <- (inddo.list[[i]]$INDEX - 
                            mean(inddo.list[[i]]$INDEX, na.rm=T)) /
    sd(inddo.list[[i]]$INDEX, na.rm=T)
  
  inddo.list[[i]]$CV.Z <- (inddo.list[[i]]$CV - 
                            mean(inddo.list[[i]]$CV, na.rm=T)) /
    sd(inddo.list[[i]]$CV, na.rm=T)
}
indd <- do.call(rbind, inddo.list)
row.names(indd) <- NULL

# Plotting
inddo.list <- split(indd, f=indd$STOCK)
namelist <- c("FALL", "SPRING", "SUMMER", "WINTER", "ALL YEAR")

for(i in 1:length(inddo.list)){
  temp <- inddo.list[[i]]
  temp.split <- split(temp, f=temp$SEASON)
  temp.split <- temp.split[c(namelist)]
  temp.split <- Filter(function(dt) length(dt) > 1, temp.split)
  
  inddo.list[[i]] <- temp.split
  
  rm(temp, temp.split)
}

stockseason.ts(inddo.list)

rm(namelist, inddo.list)
```
  
|        We will assume that the time series are not seasonal, since all but one index is created by grouping catch information within seasonal periods. Therefore, periodic influence in the time series should be limited. The plots of the time series support this assumption.
\smallskip
|        Some time series look to have moving averages, i.e. decreasing average abundance through time. Others look to have time-influenced variance. Both these features would make a time series non-stationary, which would need to be addressed in ARIMA modeling. Stationarity of a time series can be statistically determined using a unit root test-- we will use both the Augmented Dickey-Fuller (\texttt{adf.test()}) and Kwiatkowski-Phillips-Schmidt-Shin test \texttt{kpss.test()}. Note that the null hypothesis of the ADF test is \textbf{NON-STATIONARITY}. Therefore, p-values less than the significance level of $\alpha$ =0.05 reject the null of non-stationarity and can assume the alternative of stationarity. The opposite is true for the KPSS test-- the null hypothesis is stationarity and p-values less than the significance level of $\alpha$ =0.05 can assume the alternative of non-stationarity.
\smallskip
|        It is possible for the tests to come to "opposite" conclusions. This is because the ADF test tests for difference stationarity and the KPSS test tests for trend stationarity. If the ADF test indicates non-stationarity and the KPSS test indicates stationarity, this is evidence that the time series can be made stationary through de-trending. If the KPSS test indicates non-stationarity and the ADF test indicates stationarity, this is evidence that the time series can be made stationary through differencing. When ARIMA modeling with \texttt{auto.arima()} is implemented, any problems with stationarity will be dealt with via de-trending or differencing as needed.
\smallskip
|        Few time series have enough evidence to be considered stationarity (Table \@ref(tab:tabl-1)). These indices do not have moving averages, nor is there autocorrelation between the time lags of the index. They are similar to what we would see from a white noise process. Most time series are non-stationary. This makes sense given the colloquial wisdom of declining cod populations.

\newpage

```{r tabl-1, echo=F, warning=F, message=F, eval=T}
indd.list <- split(indd, f=indd$NAME)

# Remove indices with fewer than 4 data points (cannot do z-score, really)
indd.list <- Filter(function(dt) nrow(dt) > 4, indd.list)

# Remove from dataframe
indd <- indd[indd$NAME %in% names(indd.list),]

# Convert to time series
indd.ts <- indd.list
names(indd.ts) <- names(indd.list)
for(i in 1:length(indd.list)){
  indd.ts[[i]] <- ts(indd.list[[i]]$INDEX.Z)
}

# Create blank Augmented Dickey-Fuller list
adf.list <- vector(mode='list', 
                   length=length(indd.ts))
names(adf.list) <- names(indd.ts)

for(i in 1:length(indd.ts)){
  p.val.adf <- adf.test(na.remove(indd.ts[[i]]))
  p.val.kpss <- kpss.test(na.remove(indd.ts[[i]]), null="Level")
  Years  <- length(na.remove(indd.ts[[i]]))
  if(is.na(p.val.adf$p.value) == TRUE){
    ADF.sig <- 'NaN'
  }
  if(is.na(p.val.adf$p.value) == FALSE & p.val.adf$p.value < 0.05){
    ADF.sig <- 'Stationary'
  }
  if(is.na(p.val.adf$p.value) == FALSE & p.val.adf$p.value >= 0.05){
    ADF.sig <- 'Non-Stationary'
  }
  
  if(is.na(p.val.kpss$p.value) == TRUE){
    KPSS.sig <- 'NaN'
  }
  if(is.na(p.val.kpss$p.value) == FALSE & p.val.kpss$p.value >= 0.05){
    KPSS.sig <- 'Stationary'
  }
  if(is.na(p.val.kpss$p.value) == FALSE & p.val.kpss$p.value < 0.05){
    KPSS.sig <- 'Non-Stationary'
  }
  
  if(ADF.sig == "Non-Stationary" & KPSS.sig=="Non-Stationary"){
    Results <- "Non-stationary"
  }
  if(ADF.sig == "Stationary" & KPSS.sig == "Non-Stationary"){
    Results <- "Difference stationary"
  }
  if(ADF.sig == "Non-Stationary" & KPSS.sig == "Stationary"){
    Results <- "Trend Stationary"
  }
  if(ADF.sig=='Stationary' & KPSS.sig == "Stationary"){
    Results <- "Stationary"
  }
  
  adf.list[[i]] <- c(Years, Results)
}

adf.list <- do.call(rbind, adf.list)
adf.list <- as.data.frame(adf.list)
adf.list$Index <- rownames(adf.list)
rownames(adf.list) <- NULL
colnames(adf.list) <- c('Years',
                        'Results', 
                        'Index')
#adf.list$p.value <- round(as.numeric(adf.list$p.value), 3)

adf.order <- dplyr::select(adf.list, Index, Years, Results)

adf.order %>% 
  knitr::kable(
    format = "latex",
    align = "l",
    booktabs = TRUE,
    longtable = TRUE,
    linesep = "",
    caption = "Unit root test results",
    ) %>%
  kableExtra::kable_styling(
      position = "center",
      latex_options = c("striped", "repeat_header"),
      stripe_color = "gray!10"
    ) %>% 
  row_spec(0, background="gray", color="black",
           bold=T)

rm(adf.order, inddo, p.val.adf, p.val.kpss, i, lago, numb, sig, Years, adf.list,
   ADF.sig, KPSS.sig, Results)

```

## Autocorrelation 
  
|        The auto-correlation and partial auto-correlation functions (ACF and PACF, respectively) will be used to illustrate the correlation within each time series to itself. The blue dashed lines are the 95% significance thresholds. For ACF plots, deviations above or below the resulting envelope are an indication of temporal correlation between observations that lag value apart (in other words, a moving average). For PACF plots, deviations outside the significance thresholds indicate significant autoregressive correlation at that lag value.  
  
```{r, echo=F, warning=F, message=F, eval=T, fig.height=2, results='asis'}
for(i in 1:length(indd.ts)){
  ga <- ggAcf((indd.ts[[i]]),
              type="correlation",
              na.action=na.pass) +
      theme(panel.grid.major = element_blank(), 
            panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black"),
            legend.position = "n",
            plot.title=element_blank())
  
  gp <- ggPacf(indd.ts[[i]],
               type="correlation",
               na.action=na.pass) +
      theme(panel.grid.major = element_blank(), 
            panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black"),
            legend.position = "n",
            plot.title=element_blank())
  
  plotsgrid <- grid.arrange(ga, gp,
                           ncol=2, 
                           top=paste(names(indd.ts[i])))
  
}

rm(ga, gp, plotsgrid, i)
```

## Takeaways
  
|        The ACF and PACF plots support the results of the ADF/KPSS tests. In both the ACF and PACF plots, correlation exceeding the significance levels at any lag value is evidence of autocorrelation (self-similarity) at a time difference equal to the lag value. Evidence of autocorrelation mostly occurs at short lag values (1-3 years). Intuitively, this makes sense. Abundance of cod in any year should be similar to the abundance of cod in proximate years.

\newpage

# Seasonal correlations
  
|        \texttt{auto.arima()} was used to model all time series. This method is much faster and more accurate than implementing and comparing the AIC of ARIMA models by hand or by "eyeballing" the correct AR and MA terms from the ACF and PACF plots. Further, \texttt{auto.arima()} detects and removes problems with stationarity by de-trending or differencing the input time series as necessary. Residuals of the ARIMA models are then used in the correlation analysis to avoid incorrectly interpreting spurious correlation as significant. Residuals have been checked for within-series dependency using the Breusch-Godfrey Portmanteau test (Table \@ref(tab:tabl-2)), and none had enough evidence to reject the null hypothesis of within-series independence.
|        After modeling, we can test the correlation between model residuals using the cross-correlation function. The first round of testing will examine the correlation between indices created by a survey program in the same stock area, but in different seasons.

\newpage

```{r tabl-2, message=F, warning=F, echo=F, fig.pos='t', eval=T}
# ARIMA and portmanteau tests to model time series
arima.split <- vector(mode = "list", length = length(indd.ts))
portt.split <- vector(mode = "list", length = length(indd.ts))
names(arima.split) <- names(indd.ts)
names(portt.split) <- names(indd.ts)

for(i in 1:length(indd.ts)){
      arima.split[[i]] <- forecast::auto.arima(
        indd.ts[[i]],
        seasonal=FALSE, stepwise = FALSE, approximation = FALSE)
      residmod <- lm(arima.split[[i]]$residuals ~ 1)
      portt.split[[i]] <- checkresiduals(residmod, test = 'BG',
                                         plot=F)
}

test <- do.call(rbind, portt.split)
test <- as.data.frame(test)
test <- do.call(rbind, test$p.value)
test <- as.data.frame(test)
test$AREA <- row.names(test)
row.names(test) <- NULL
colnames(test) <- c('p.value', 'Index')
test <- dplyr::select(test, Index, p.value)
#test <- test[with(test, 
#                  order(p.value)),]
#rownames(test) <- NULL
test$p.value <- round(test$p.value, 3)

test$AR <- NA
test$I <- NA
test$MA <- NA

for(i in 1:nrow(test)){
  test$AR[i] <- arimaorder(arima.split[[i]])['p']
  test$I[i] <- arimaorder(arima.split[[i]])['d']
  test$MA[i] <- arimaorder(arima.split[[i]])['q']
}

test$Results <- NA
for(i in 1:nrow(test)){
  if(test$p.value[i] < 0.05){
    test$Results[i] <- 'Non-stationary'
  }
  if(test$p.value[i] >= 0.05){
    test$Results[i] <- 'Stationary'
  }
}

test <- dplyr::select(test, Index, AR, I, MA, p.value, Results)

test %>% 
  knitr::kable(
    format = "latex",
    align = "l",
    booktabs = TRUE,
    longtable = TRUE,
    linesep = "",
    caption = "Portmanteau test statistics",
    ) %>%
  kableExtra::kable_styling(
      position = "center",
      latex_options = c("striped", "repeat_header"),
      stripe_color = "gray!10"
    ) %>% 
  row_spec(0, background="gray", color="black",
           bold=T)

rm(portt.split, residmod, test, i, indd.ts)
```
  
```{r, message=F, warning=F, echo=F, eval=T}
# Create survey-stock groupings
indd$SURSTOCK <- paste(indd$SURVEY, indd$STOCK, indd$AGES)

# ARIMA and portmanteau tests to model time series
sea.split <- split(indd, f=indd$SURSTOCK)

# Split into seasons
for(i in 1:length(sea.split)){
  temp <- sea.split[[i]]
  temp.split <- split(temp, f=temp$SEASON)
  sea.split[[i]] <- temp.split
}

# Remove indices with no matching fall-spring seasons
sea.split <- Filter(function(dt) length(dt) > 1, sea.split)

# Create arima list and portmanteau list
arima.sea <- vector(mode = "list", length = length(sea.split))
portt.sea <- vector(mode = "list", length = length(sea.split))
names(arima.sea) <- names(sea.split)
names(portt.sea) <- names(sea.split)

for(i in 1:length(sea.split)){
  for(j in 1:length(sea.split[[i]])){
      arima.sea[[i]][[j]] <- forecast::auto.arima(
        ts(sea.split[[i]][[j]]$INDEX.Z,
           frequency=frequency(sea.split[[i]][[j]]$INDEX.Z),
           start=time(sea.split[[i]][[j]]$YEAR[1])),
           seasonal=FALSE, approximation=FALSE, stepwise=FALSE)
      residmod <- lm(arima.sea[[i]][[j]]$residuals ~ 1)
      portt.sea[[i]][[j]] <- checkresiduals(residmod, test = 'BG',
                                         plot=F)
  }
}

for(i in 1:length(arima.sea)){
  names(arima.sea[[i]]) <- names(sea.split[[i]])
  names(portt.sea[[i]]) <- names(sea.split[[i]])
}

resids.sea <- vector(mode = "list", length = length(arima.sea))
names(resids.sea) <- names(arima.sea)

for(i in 1:length(resids.sea)){
  survname <- names(sea.split[i])
  seanames <- names(sea.split[[i]])
  for(j in 1:length(seanames)){
    resids.sea[[i]][[j]] <- as.data.frame(
      cbind(
        sea.split[[paste0(survname)]][[paste0(seanames[j])]]$YEAR,
            as.numeric(as.numeric(
        arima.sea[[paste0(survname)]][[paste0(seanames[j])]]$residuals
            )))
    )
    colnames(resids.sea[[i]][[j]]) <- c('YEAR', 'RESIDUALS')
    resids.sea[[i]][[j]]$SURVEY <- survname
    resids.sea[[i]][[j]]$SEASON <- seanames[j]
  }
}

resids <- unlist(resids.sea, recursive = F, use.names = T)
resids <- do.call(rbind, resids)
row.names(resids) <- NULL

resids$RESIDUALS <- as.numeric(resids$RESIDUALS)

resids.sea <- split(resids, f=resids$SURVEY)

for(i in 1:length(resids.sea)){
  temp <- resids.sea[[i]]
  temp$RESIDUALS <- as.numeric(temp$RESIDUALS)
  temp.split <- split(temp, f=temp$SEASON)
  resids.sea[[i]] <- temp.split
}

for(i in 1:length(resids.sea)){
  for(j in 1:length(resids.sea[[i]])){
      resids.sea[[i]][[j]]$YEAR <- as.Date(
        paste0(resids.sea[[i]][[j]]$YEAR, "-01-01"))
      resids.sea[[i]][[j]] <- pad(resids.sea[[i]][[j]], interval='year',
                    start_val = as.Date("1982-01-01"),
                    end_val = as.Date('2022-01-01'))
      resids.sea[[i]][[j]]$SURVEY <- levels(as.factor(resids.sea[[i]][[j]]$SURVEY))
      resids.sea[[i]][[j]]$SEASON <- levels(as.factor(
        resids.sea[[i]][[j]]$SEASON))
  }
}

resids.s.split <- resids.sea

for(i in 1:length(resids.s.split)){
  temp <- resids.sea[[i]]
  temp2 <- as.data.frame(temp[[1]]$RESIDUALS)
  if(length(temp) > 1){
    for(j in 2:length(temp)){
      temp2 <- cbind(temp2, temp[[j]]$RESIDUALS)
    }
  }
  names(temp2) <- names(temp)
  rownames(temp2) <- seq(1982,2022,1)
  resids.s.split[[i]] <- temp2
}

inddo <- subset(indd, AGES!="A0")
inddo.list <- split(inddo, f=inddo$SURSTOCK)

rm(portt.sea, residmod, resids, resids.sea, sea.split, temp, temp.split,
   temp2, i, j, seanames, survname, arima.sea)
```

## Mass DMF Trawl WGOM
  
```{r, echo=F, warning=F, message=F, eval=T, fig.width=7, fig.height=9}
input <- "MADMF Trawl WGOM A1PLUS"
temp <- resids.s.split[[paste0(input)]]

seasonal.cors(temp, input)
  
```
  
## ME-NH Inshore Trawl EGOM
  
```{r, echo=F, warning=F, message=F, eval=T, fig.width=7, fig.height=9}
input <- "ME-NH Inshore Trawl EGOM A0+"
temp <- resids.s.split[[paste0(input)]]

seasonal.cors(temp, input)
  
```
  
## ME-NH Inshore Trawl WGOM
  
```{r, echo=F, warning=F, message=F, eval=T, fig.width=7, fig.height=9}
input <- "ME-NH Inshore Trawl WGOM A0+"
temp <- resids.s.split[[paste0(input)]]

seasonal.cors(temp, input)
  
```
  
## NEFSC BLLS WGOM
  
```{r, echo=F, warning=F, message=F, eval=T, fig.width=7, fig.height=9}
input <- "NEFSC BLLS WGOM A0+"
temp <- resids.s.split[[paste0(input)]]

seasonal.cors(temp, input)
  
```
  
## NEFSC BTS EGOM
  
```{r, echo=F, warning=F, message=F, eval=T, fig.width=7, fig.height=9}
input <- "NEFSC BTS EGOM A0+"
temp <- resids.s.split[[paste0(input)]]

seasonal.cors(temp, input)
  
```
  
## NEFSC BTS GBK
  
```{r, echo=F, warning=F, message=F, eval=T, fig.width=7, fig.height=9}
input <- "NEFSC BTS GBK A0+"
temp <- resids.s.split[[paste0(input)]]

seasonal.cors(temp, input)
  
```
  
## NEFSC BTS SNE
  
```{r, echo=F, warning=F, message=F, eval=T, fig.width=7, fig.height=9}
input <- "NEFSC BTS SNE A0+"
temp <- resids.s.split[[paste0(input)]]

seasonal.cors(temp, input)
  
```
  
## NEFSC BTS WGOM
  
```{r, echo=F, warning=F, message=F, eval=T, fig.width=7, fig.height=9}
input <- "NEFSC BTS WGOM A0+"
temp <- resids.s.split[[paste0(input)]]

seasonal.cors(temp, input)
  
```
  
## Takeaways
  
|        The cross-correlation function gives correlation values between indices _x_ and _y_ at all possible time lags _t_. At positive lag values, index _x_ lags index _y_ (changes are first seen in index _y_). At negative lag values, index _x_ leads index _y_ (changes are first seen in index _x_).
\smallskip
|        In this analysis, Fall indices are always index _x_ and Spring indices are always index _y_. This has implications to the way in which the cross-correlation plots can be interpreted (Figure @ref(fig:lags)). At lag 0, Fall of year _t_ is compared to Spring of year _t_. At lag 1, Fall of year _t_~+1~ is compared to Spring of year _t_ (three seasons behind). At lag -1, Fall of year _t_~-1~ is compared to Spring of year _t_. We expect strong autoregressive autocorrelation for temporally proximate values, as indices closer together in time are more likely to be similar. Therefore, we would expect significant correlation values at low lag values (absolute value of lag is < 2).
<center>
\begin{figure}[H]
  \includegraphics{lag_illustration.png}
  \caption{Illustration of seasonal implications of cross-correlation lag values assuming year \texttt{t} is 2000.}
  \label{fig:lags}
\end{figure}
</center>
|        Eight surveys had data to support indices for both the fall and spring seasons in the same stock area. Five of these surveys did not have significant cross-correlation between their fall and spring indices at any lag value. 
\smallskip
|        The ME-NH Inshore trawl WGOM fall and spring indices were significantly correlated at a lag value of 0, indicating that fall indices can be predicted by the spring indices completed in the same year. The NEFSC SNE bottom trawl indices fall and spring are significantly correlated at a lag value of 1, indicating that fall indices can be predicted by the spring indices of the year prior.
\smallskip
|        There are some surveys that have correlations between their fall and spring indices at lag values greater than 1 or less than -1. The Mass. DMF WGOM survey and the ME-NH Inshore WGOM survey have strong correlations at lag values of 4 and -4, respectively. These correlations are unusual and should be explored further, as they may be the results of an unexplored third correlate. The NEFSC BTS SNE survey has a strong correlation at a lag value of 16, though this value is so extreme that it may be the result of some spurious correlation remaining in the models.
  
\newpage

# Stock-Season Groupings
  
|        The second round of testing will examine the correlation between indices created by different survey programs, but in the same stock-season grouping.
  
```{r, message=F, warning=F, echo=F, eval=T}
rm(resids.s.split, input)

# Remove indices with broken time series
arima.split2 <- Filter(function(dt) length(dt) > 1, arima.split)
arima.split2["MADMF Trawl SPRING WGOM A0"] <- NULL
arima.split2["MADMF Trawl SPRING SNE A0"] <- NULL
arima.split2["MADMF Trawl FALL WGOM A0"] <- NULL

resids.split <- vector(mode = "list", length = length(arima.split2))
names(resids.split) <- names(arima.split2)
for(i in 1:length(resids.split)){
  areaname <- names(resids.split[i])
  resids.split[[i]] <- as.data.frame(cbind(
    indd.list[[paste0(areaname)]]$YEAR,
    as.numeric(as.numeric(arima.split2[[paste0(areaname)]]$residuals))))
  colnames(resids.split[[i]]) <- c('YEAR', 'RESIDUALS')
  resids.split[[i]]$NAME <- indd.list[[paste0(areaname)]]$NAME
  resids.split[[i]]$STOCKSEASON <- indd.list[[paste0(areaname)]]$STOCKSEASON
}
resids <- do.call(rbind, resids.split)
row.names(resids) <- NULL
resids$RESIDUALS <- as.numeric(resids$RESIDUALS)

resids.split <- split(resids, f=resids$STOCKSEASON)

for(i in 1:length(resids.split)){
  temp <- resids.split[[i]]
  temp$RESIDUALS <- as.numeric(temp$RESIDUALS)
  temp.split <- split(temp, f=temp$NAME)
  resids.split[[i]] <- temp.split
}

for(i in 1:length(resids.split)){
  for(j in 1:length(resids.split[[i]])){
      #tempp <- temp[[j]]
      resids.split[[i]][[j]]$YEAR <- as.Date(paste0(
        resids.split[[i]][[j]]$YEAR, "-01-01"))
      resids.split[[i]][[j]] <- pad(resids.split[[i]][[j]], interval='year',
                    start_val = as.Date("1982-01-01"),
                    end_val = as.Date('2022-01-01'))
      resids.split[[i]][[j]]$NAME <- levels(as.factor(resids.split[[i]][[j]]$NAME))
      resids.split[[i]][[j]]$STOCKSEASON <- levels(as.factor(
        resids.split[[i]][[j]]$STOCKSEASON))
  }
}

resids.ss.split <- resids.split

for(i in 1:length(resids.ss.split)){
  temp <- resids.split[[i]]
  temp2 <- as.data.frame(temp[[1]]$RESIDUALS)
  if(length(temp) > 1){
    for(j in 2:length(temp)){
      temp2 <- cbind(temp2, temp[[j]]$RESIDUALS)
    }
  }
  names(temp2) <- names(temp)
  rownames(temp2) <- seq(1982,2022,1)
  resids.ss.split[[i]] <- temp2
}

resids.ss.split <- Filter(function(dt) length(dt) > 1, resids.ss.split)

inddo.list <- split(inddo, f=inddo$STOCKSEASON)

rm(arima.split, arima.split2, resids, resids.split, temp, temp.split,
   temp2, areaname, i, j, indd.list, inddo)

```
  
## Eastern Gulf of Maine - Fall
  
```{r, echo=F, warning=F, message=F, eval=T, fig.width=7, fig.height=9, results='asis'}

input <- "EGOM FALL"
temp <- resids.ss.split[[paste0(input)]]

stockseason.cors(temp, input)

```
  
## Eastern Gulf of Maine - Spring
  
```{r, echo=F, warning=F, message=F, eval=T, fig.width=7, fig.height=9, results='asis'}

input <- "EGOM SPRING"
temp <- resids.ss.split[[paste0(input)]]

stockseason.cors(temp, input)

```
  
## Georges Bank - Spring
  
```{r, echo=F, warning=F, message=F, eval=T, fig.width=7, fig.height=9, results='asis'}

input <- "GBK SPRING"
temp <- resids.ss.split[[paste0(input)]]

stockseason.cors(temp, input)

```
  
## Western Gulf of Maine - Fall
  
```{r, echo=F, warning=F, message=F, eval=T, fig.width=7, fig.height=9, results='asis'}

input <- "WGOM FALL"
temp <- resids.ss.split[[paste0(input)]]

stockseason.cors(temp, input)

```
  
## Western Gulf of Maine - Spring
  
```{r, echo=F, warning=F, message=F, eval=T, fig.width=7, fig.height=9, results='asis'}

input <- "WGOM SPRING"
temp <- resids.ss.split[[paste0(input)]]

stockseason.cors(temp, input)

```
  
## Takeaways
  
|        Since all these comparisons are of indices generated by surveys conducted in the same stock area in the same season, we would expect to see significant correlations at lag 0 for surveys. This may not be true for comparisons of indices generated by surveys using different methods (i.e, NEFSC Bottom Longline surveys vs. NEFSC Bottom Trawl surveys). The varied methods and habitat type sampling of the EGOM Sentinel jigging and NEFSC Bottom Longline surveys may affect the index values and likely preclude them from correlating to the trawl surveys.
\smallskip
|        In practice, 8 of the 20 stock-season comparisons have no significant correlations at any lag value. Four comparisons have significant correlations at lag 0: all of these are trawl survey-trawl survey comparisons.
\smallskip
|        Five comparisons have significant correlations at lag 1 or -1. In the Eastern Gulf of Maine, the spring ME-NH inshore trawl index is correlated to the NEFSC BTS index at a lag value of -1, indicating that the ME-NH trawl index leads the NEFSC BTS by a year. This is also true of the ME-NH Inshore trawl and NEFSC BTS indices for the Western Gulf of Maine in the fall. Several spring Western Gulf of Maine indices are correlated at lag values of 1 or -1: the MADMF Industry and ME-NH Inshore trawl indices are correlated at lag values 0-2, the MADMF Industry and NEFSC BTS indices are correlated at a lag value of -1, and finally, the ME-NH Inshore trawl and NEFSC BLLS indices are correlated at a lag value of 1.
\smallskip
|        Significant correlations at lag values greater than 1 or less than -1 are not easily explained. It is possible that a third variable, unaccounted for in the cross-correlation function, is driving the correlation at higher lags. Further exploration would be needed to propose a mechanism for these high-lag correlations. Six of the 21 comparisons have significant correlations at lag values greater than 1 or less than -1.
\smallskip
|        The two non-trawl survey indices (EGOM Sentinel and NEFSC BLLS) have few correlations to the other surveys. The EGOM Sentinel jigging index is not correlated to any other survey. The NEFSC BLLS index has correlations to three other indices in the Western Gulf of Maine in the spring; the ME-NH trawl index at a lag value of 1, the MADMF Industry index at a lag value of -2, and the NEFSC BTS at a lag value of 3. These correlations may be spurious due to the short length of the NEFSC BLLS time series (eight years).

# Considerations and limitations

|        There are several caveats to this attempt at time series analysis of cod indices of abundance. First, short time series are difficult to compare due to small sample sizes. Time series with fewer than 4 years of continuous data could not be included. Comparisons of time series with fewer than 10 years of temporal overlap could also result in spurious correlations that are not reflective of the true nature of their relationship.
\smallskip
|        Indices that had non-continuous data also present a challenge-- missing values cannot reliably be imputed from the average of the series, as demonstrated by the ACF and PACF plots. But neither can the time series be made continuous simply by removing missing values and concatenating the remainder into a continuous time series, as this approach would make distant observations appear to be unrealistically close. For this analysis, the \texttt{na.pass} argument was used to interpolate missing values in the time series. This choice likely introduced spurious, non-realistic correlation. This is especially true of time series that have large stretches of missing data, such as the MADMF Industry Trawl survey.
\smallskip
|        The definition of seasons is somewhat flexible by survey. For example, the EGOM Sentinel survey considers June to be within the "Fall" season. Most other surveys, if they sample in June, count it within the "Spring" survey. As the stratified indices were provided by individual survey management groups, seasonal indices have not been standardized to specific months. Standardizing the boundaries of each season would help to strengthen and more clearly present correlations between indices.
\smallskip
|        Similarly, survey efforts vary in length of time spent sampling. For example, the MADMF Trawl Spring survey occurs only in April through June. However, the NEFSC BTS spring survey occurs February through June. It is possible that the wider temporal window of the NEFSC BTS survey could capture different signals than the MADMF Trawl survey, or that it could dampen the same signals that are more clearly present in the MADMF Trawl survey.
\smallskip
|        Finally, the indices also include a spatial aspect not considered in this time series analysis. As indicated in the included spatial plots, several indices that take place in the same stock area do not actually overlap in space. This is particularly true when comparing surveys that use different gear types (i.e. a trawl survey vs. a jigging survey). If the habitats sampled by two compared surveys are sufficiently different (i.e., trawlable mud vs. non-trawlable cobble, very shallow vs. very deep), it is possible that the populations of cod will be significantly different and the time series will not be correlated. Addressing the spatial and habitat aspect of these data will require its own thorough analysis using a method other than time series analysis.
